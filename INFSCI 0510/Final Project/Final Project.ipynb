{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Most Optimal Parking Location Based on Time of Day And Location\n",
    "\n",
    "Ethan Defilippi\n",
    "\n",
    "ECD57@pitt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Attending college at the University of Pittsburgh can be a challenge if you are a\n",
    "commuter with early classes. It is not an uncommon occurrence to spend minutes looking for\n",
    "parking once you arrive on campus. I plan on answering the question, “Where is the best place\n",
    "to park on Pitt’s campus based on time of day?” Using data from the Pennsylvania Department\n",
    "of Transportation and the Pittsburgh Parking Authority, I plan on training a model to predict the\n",
    "best meter location based on what time of day and what location within the city of Pittsburgh to\n",
    "park at. I consider this project useful because it helps to alleviate a real-world problem that\n",
    "many people face (especially Pitt commuters). Anyone who commutes to work or school and\n",
    "has trouble finding parking could benefit from this model. I plan on using two data sets that\n",
    "contain meter information for almost all parking meters within the city of Pittsburgh and traffic\n",
    "density information throughout the city at all times of day. I plan on training a Random Forest\n",
    "model on the datasets to accomplish this because of their resistance to missing data and their\n",
    "ability to output multiple options for parking with a probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import display, clear_output\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "clicked_coords = []\n",
    "\n",
    "traffic = pd.read_csv('data-trafficcounts.csv')\n",
    "meters = pd.read_csv('parking-meters.csv')\n",
    "## Drop all column in meters, but x,y, and location\n",
    "meters = meters[['x', 'y', 'location','rate']]\n",
    "##Drop rows with NA values if x or y is NA\n",
    "meters = meters.dropna(subset=['x', 'y','rate','location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of NA's\n",
    "\n",
    "My first data set had ZERO NA values in it (AMAZING!). The second data set (the parking meters one) had a few NA's for x and y. These are extremely important, so those rows were dropped entirely. They are coordinates and there is no way to bring them back into the data or estimate them. The NA's from created_user/data and last_edited_user/data are not important to the data set, so they are kept in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dealing with outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots for before/after comparison\n",
    "fig, axes = plt.subplots(2, 6, figsize=(20, 8))\n",
    "fig.suptitle('Distribution Before and After Outlier Removal', fontsize=16)\n",
    "\n",
    "# Lists to store test results\n",
    "before_tests = []\n",
    "after_tests = []\n",
    "\n",
    "hours = ['7a', '8a', '9a', '10a', '11a', '12p']\n",
    "\n",
    "# Before outlier removal\n",
    "for idx, hour in enumerate(hours):\n",
    "    # Store normality test results\n",
    "    stat, p_value = stats.normaltest(traffic[hour])\n",
    "    before_tests.append({'hour': hour, 'p_value': p_value})\n",
    "    \n",
    "    # Plot distribution\n",
    "    sns.histplot(data=traffic[hour], kde=True, ax=axes[0, idx])\n",
    "    axes[0, idx].set_title(f'{hour}\\np={p_value:.3f}')\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "traffic_clean = traffic.copy()\n",
    "for hour in hours:\n",
    "    Q1 = traffic[hour].quantile(0.25)\n",
    "    Q3 = traffic[hour].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    traffic_clean[hour] = traffic_clean[hour].clip(lower=lower, upper=upper)\n",
    "\n",
    "# After outlier removal\n",
    "for idx, hour in enumerate(hours):\n",
    "    # Store normality test results\n",
    "    stat, p_value = stats.normaltest(traffic_clean[hour])\n",
    "    after_tests.append({'hour': hour, 'p_value': p_value})\n",
    "    \n",
    "    # Plot distribution\n",
    "    sns.histplot(data=traffic_clean[hour], kde=True, ax=axes[1, idx])\n",
    "    axes[1, idx].set_title(f'{hour}\\np={p_value:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test results\n",
    "print(\"\\nNormality Test Results (p-values):\")\n",
    "print(\"Hour\\tBefore\\tAfter\")\n",
    "for b, haversine_component in zip(before_tests, after_tests):\n",
    "    print(f\"{b['hour']}\\t{b['p_value']:.3f}\\t{haversine_component['p_value']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose not to remove any outliers from the data-trafficcounts.csv file. When cleaning outliers, I used IQR. From the visualizations shown, there does not seem to have been any major outliers because the data is largely unchanged. Outliers here could represent sensor errors, but there seem to have been none present. (Great job PennDot). The other dataset has no outliers to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard visualiztions do not show great correlations with this data because almost all of the meter values are independent to one another. All sensors in my feature list seem to be very positively skewed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to use all of the traffic hours as features from the trafficcounts dataset in order to find real world traffic counts at important commute times for people. I also plan on using all of the meter GPS coordinates to train the model on meters close to each other. The response features for this model will be the longitudes and latitudes of parking zones within the city in order to give users a parking area to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fandom Forest Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my first model, I used a random forest model because they are great with non-linear data (such as this data) and output the confidence ratings that I need for the interactive map. Random forest also handles skewed data very well, so it was the perfect choice for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandForestRegression:\n",
    "    def __init__(self):\n",
    "        self.traffic_data = None\n",
    "        self.meter_data = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.confidence_threshold = 0.6  # Minimum confidence to show recommendation\n",
    "        \n",
    "    def load_data(self, traffic_df, meters_df):\n",
    "        \"\"\"Load and prepare the traffic and parking meter datasets\"\"\"\n",
    "        self.traffic_data = traffic_df\n",
    "        self.meter_data = meters_df\n",
    "        \n",
    "        self.time_columns = ['1a','2a','3a','4a','5a','6a','7a', '8a', '9a', '10a', '11a', '12p','1p','2p','3p','4p','5p','6p','7p','8p','9p','10p','11p','12a']\n",
    "        \n",
    "        print(f\"Loaded {len(self.traffic_data)} traffic sensors and {len(self.meter_data)} parking meters\")\n",
    "        \n",
    "    def calculate_distance(self, start_latitude, start_longitude, destination_latitude, destination_longitude):\n",
    "        \"\"\"\n",
    "        Calculate distance between two points using Haversine formula\n",
    "        \n",
    "        a = sin²(φB - φA/2) + cos φA * cos φB * sin²(λB - λA/2)\n",
    "        c = 2 * atan2( √a, √(1−a) )\n",
    "        d = R ⋅ c\n",
    "        \n",
    "        \"\"\"     \n",
    "        r = 6371  # Earth's radius in kilometers\n",
    "\n",
    "        start_latitude, start_longitude, destination_latitude, destination_longitude = map(math.radians, [start_latitude, start_longitude, destination_latitude, destination_longitude])\n",
    "        delta_latitude = destination_latitude - start_latitude\n",
    "        delta_longitude = destination_longitude - start_longitude\n",
    "        \n",
    "        haversine_component = math.sin(delta_latitude/2)**2 + math.cos(start_latitude) * math.cos(destination_latitude) * math.sin(delta_longitude/2)**2\n",
    "        central_angle = 2 * math.asin(math.sqrt(haversine_component))\n",
    "        return r * central_angle\n",
    "\n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare features for the machine learning model\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, meter in self.meter_data.iterrows():\n",
    "            # Find nearest traffic sensors\n",
    "            distances = []\n",
    "            traffic_values = []\n",
    "            \n",
    "            for _, sensor in self.traffic_data.iterrows():\n",
    "                dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                            sensor['Latitude'], sensor['Longitude'])\n",
    "                distances.append(dist)\n",
    "                \n",
    "                # Get traffic values for all time periods\n",
    "                time_values = [sensor[time_col] for time_col in self.time_columns]\n",
    "                traffic_values.append(time_values)\n",
    "            \n",
    "            # Get 3 nearest sensors\n",
    "            nearest_indices = np.argsort(distances)[:3]\n",
    "            \n",
    "            for time_idx, time_col in enumerate(self.time_columns):\n",
    "                # Features for this time period\n",
    "                feature_row = [\n",
    "                    meter['x'],  # longitude\n",
    "                    meter['y'],  # latitude\n",
    "                    distances[nearest_indices[0]],  # distance to nearest sensor\n",
    "                    distances[nearest_indices[1]],  # distance to 2nd nearest\n",
    "                    distances[nearest_indices[2]],  # distance to 3rd nearest\n",
    "                    traffic_values[nearest_indices[0]][time_idx],  # nearest sensor traffic\n",
    "                    traffic_values[nearest_indices[1]][time_idx],  # 2nd nearest traffic\n",
    "                    traffic_values[nearest_indices[2]][time_idx],  # 3rd nearest traffic\n",
    "                    time_idx  # time of day encoded as 0-5\n",
    "                ]\n",
    "                \n",
    "                features.append(feature_row)\n",
    "                \n",
    "                # Label: inverse of average traffic (higher is better for parking)\n",
    "                avg_traffic = np.mean([traffic_values[i][time_idx] for i in nearest_indices])\n",
    "                parking_score = 1.0 / (1.0 + avg_traffic/100)  # Normalize to 0-1\n",
    "                labels.append(parking_score)\n",
    "        \n",
    "        return np.array(features), np.array(labels)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the Random Forest model\"\"\"\n",
    "        print(\"Preparing features and training model...\")\n",
    "        \n",
    "        X, y = self.prepare_features()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "    def predict_parking_spots(self, target_lat, target_lon, time, max_distance=1.0):\n",
    "        \"\"\"\n",
    "        Predict best parking spots and their confidence scores\n",
    "        \n",
    "        Parameters:\n",
    "        - target_lat: float, destination latitude\n",
    "        - target_lon: float, destination longitude\n",
    "        - time: str, time of day (e.g., '8a')\n",
    "        - max_distance: float, maximum walking distance in km\n",
    "        \n",
    "        Returns: DataFrame with predictions and confidence scores\n",
    "        \"\"\"\n",
    "        if time not in self.time_columns:\n",
    "            raise ValueError(f\"Time must be one of {self.time_columns}\")\n",
    "        \n",
    "        time_idx = self.time_columns.index(time)\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_features = []\n",
    "        valid_meters = []\n",
    "        \n",
    "        for idx, meter in self.meter_data.iterrows():\n",
    "            # Calculate distance to destination\n",
    "            dist = self.calculate_distance(target_lat, target_lon, meter['y'], meter['x'])\n",
    "            \n",
    "            if dist <= max_distance:\n",
    "                # Find nearest traffic sensors\n",
    "                sensor_distances = []\n",
    "                traffic_values = []\n",
    "                \n",
    "                for _, sensor in self.traffic_data.iterrows():\n",
    "                    sensor_dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                                        sensor['Latitude'], sensor['Longitude'])\n",
    "                    sensor_distances.append(sensor_dist)\n",
    "                    traffic_values.append(sensor[time])\n",
    "                \n",
    "                # Get 3 nearest sensors\n",
    "                nearest_indices = np.argsort(sensor_distances)[:3]\n",
    "                \n",
    "                feature_row = [\n",
    "                    meter['x'],\n",
    "                    meter['y'],\n",
    "                    sensor_distances[nearest_indices[0]],\n",
    "                    sensor_distances[nearest_indices[1]],\n",
    "                    sensor_distances[nearest_indices[2]],\n",
    "                    traffic_values[nearest_indices[0]],\n",
    "                    traffic_values[nearest_indices[1]],\n",
    "                    traffic_values[nearest_indices[2]],\n",
    "                    time_idx\n",
    "                ]\n",
    "                \n",
    "                pred_features.append(feature_row)\n",
    "                valid_meters.append(idx)\n",
    "        \n",
    "        if not pred_features:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Scale features and make predictions\n",
    "        X_pred = self.scaler.transform(np.array(pred_features))\n",
    "        predictions = self.model.predict(X_pred)\n",
    "        \n",
    "        # Get confidence scores using tree variance\n",
    "        confidences = []\n",
    "        for x in X_pred:\n",
    "            tree_predictions = [tree.predict(x.reshape(1, -1))[0] for tree in self.model.estimators_]\n",
    "            confidence = 1 - np.std(tree_predictions)  # Lower variance = higher confidence\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = self.meter_data.iloc[valid_meters].copy()\n",
    "        results['prediction'] = predictions\n",
    "        results['confidence'] = confidences\n",
    "        results['distance_to_dest'] = [\n",
    "            self.calculate_distance(target_lat, target_lon, row['y'], row['x'])\n",
    "            for _, row in results.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Filter by confidence threshold and sort\n",
    "        results = results[results['confidence'] >= self.confidence_threshold]\n",
    "        results = results.sort_values('prediction', ascending=False)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def create_map(self, target_lat, target_lon, recommendations):\n",
    "        \"\"\"Create a folium map with the recommended parking spots\"\"\"\n",
    "        # Create base map centered on target location\n",
    "        m = folium.Map(location=[target_lat, target_lon], zoom_start=15)\n",
    "        \n",
    "        # Add destination marker\n",
    "        folium.Marker(\n",
    "            [target_lat, target_lon],\n",
    "            popup='Destination',\n",
    "            icon=folium.Icon(color='red', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add recommended parking spots\n",
    "        for _, spot in recommendations.iterrows():\n",
    "            # Color based on confidence (green=high, orange=not high)\n",
    "            color = 'green' if spot['confidence'] > 0.7 else 'orange'\n",
    "            \n",
    "            # Create popup content\n",
    "            popup_html = f\"\"\"\n",
    "                <b>{spot['location']}</b><br>\n",
    "                Rate: {spot['rate']}<br>\n",
    "                Distance: {spot['distance_to_dest']:.2f} km<br>\n",
    "                Confidence: {spot['confidence']:.2f}<br>\n",
    "                Score: {spot['prediction']:.2f}\n",
    "            \"\"\"\n",
    "\n",
    "            folium.Marker(\n",
    "                [spot['y'], spot['x']],\n",
    "                popup=popup_html,\n",
    "                icon=folium.Icon(color=color, icon='parking', prefix='fa')\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add search box\n",
    "        plugins.Geocoder().add_to(m)\n",
    "        \n",
    "        # Add layer control\n",
    "        folium.LayerControl().add_to(m)\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def create_interactive_map(self):\n",
    "        global clicked_coords\n",
    "        clicked_coords = []\n",
    "        \n",
    "        # Create base map focused on Pittsburgh\n",
    "        m = folium.Map(\n",
    "            location=[40.4406, -79.9959],\n",
    "            zoom_start=12\n",
    "        )\n",
    "        \n",
    "        m.add_child(folium.LatLngPopup())\n",
    "        # Add clickable marker functionality\n",
    "        m.add_child(folium.ClickForLatLng())\n",
    "        \n",
    "        return m\n",
    "\n",
    "RandForest = RandForestRegression()\n",
    "\n",
    "# Load data\n",
    "RandForest.load_data(traffic, meters)\n",
    "\n",
    "# Train model\n",
    "RandForest.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my second model, I used a decision tree model because they are outlier resistant, handle non-linear data well, and handle missing values well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.traffic_data = None\n",
    "        self.meter_data = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.confidence_threshold = 0.6  # Minimum confidence to show recommendation\n",
    "        \n",
    "    def load_data(self, traffic_df, meters_df):\n",
    "        \"\"\"Load and prepare the traffic and parking meter dataframes\"\"\"\n",
    "        self.traffic_data = traffic_df\n",
    "        self.meter_data = meters_df\n",
    "        \n",
    "        self.time_columns = ['1a','2a','3a','4a','5a','6a','7a', '8a', '9a', '10a', '11a', '12p','1p','2p','3p','4p','5p','6p','7p','8p','9p','10p','11p','12a']\n",
    "        \n",
    "        print(f\"Loaded {len(self.traffic_data)} traffic sensors and {len(self.meter_data)} parking meters\")\n",
    "        \n",
    "    def calculate_distance(self, start_latitude, start_longitude, destination_latitude, destination_longitude):\n",
    "        \"\"\"\n",
    "        Calculate distance between two points using Haversine formula\n",
    "        \n",
    "        a = sin²(φB - φA/2) + cos φA * cos φB * sin²(λB - λA/2)\n",
    "        c = 2 * atan2( √a, √(1−a) )\n",
    "        d = R ⋅ c\n",
    "        \n",
    "        \"\"\"     \n",
    "        r = 6371  # Earth's radius in kilometers\n",
    "\n",
    "        start_latitude, start_longitude, destination_latitude, destination_longitude = map(math.radians, [start_latitude, start_longitude, destination_latitude, destination_longitude])\n",
    "        delta_latitude = destination_latitude - start_latitude\n",
    "        delta_longitude = destination_longitude - start_longitude\n",
    "        \n",
    "        haversine_component = math.sin(delta_latitude/2)**2 + math.cos(start_latitude) * math.cos(destination_latitude) * math.sin(delta_longitude/2)**2\n",
    "        central_angle = 2 * math.asin(math.sqrt(haversine_component))\n",
    "        return r * central_angle\n",
    "\n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare features for the machine learning model\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, meter in self.meter_data.iterrows():\n",
    "            # Find nearest traffic sensors\n",
    "            distances = []\n",
    "            traffic_values = []\n",
    "            \n",
    "            for _, sensor in self.traffic_data.iterrows():\n",
    "                dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                            sensor['Latitude'], sensor['Longitude'])\n",
    "                distances.append(dist)\n",
    "                \n",
    "                # Get traffic values for all time periods\n",
    "                time_values = [sensor[time_col] for time_col in self.time_columns]\n",
    "                traffic_values.append(time_values)\n",
    "            \n",
    "            # Get 3 nearest sensors\n",
    "            nearest_indices = np.argsort(distances)[:3]\n",
    "            \n",
    "            for time_idx, time_col in enumerate(self.time_columns):\n",
    "                # Features for this time period\n",
    "                feature_row = [\n",
    "                    meter['x'],  # longitude\n",
    "                    meter['y'],  # latitude\n",
    "                    distances[nearest_indices[0]],  # distance to nearest sensor\n",
    "                    distances[nearest_indices[1]],  # distance to 2nd nearest\n",
    "                    distances[nearest_indices[2]],  # distance to 3rd nearest\n",
    "                    traffic_values[nearest_indices[0]][time_idx],  # nearest sensor traffic\n",
    "                    traffic_values[nearest_indices[1]][time_idx],  # 2nd nearest traffic\n",
    "                    traffic_values[nearest_indices[2]][time_idx],  # 3rd nearest traffic\n",
    "                    time_idx  # time of day encoded as 0-5\n",
    "                ]\n",
    "                \n",
    "                features.append(feature_row)\n",
    "                \n",
    "                # Label: inverse of average traffic (higher is better for parking)\n",
    "                avg_traffic = np.mean([traffic_values[i][time_idx] for i in nearest_indices])\n",
    "                parking_score = 1.0 / (1.0 + avg_traffic/100)  # Normalize to 0-1\n",
    "                labels.append(parking_score)\n",
    "        \n",
    "        return np.array(features), np.array(labels)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the Decision Tree model\"\"\"\n",
    "        print(\"Preparing features and training model...\")\n",
    "        \n",
    "        X, y = self.prepare_features()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "                \n",
    "        #Train the Decision Tree model\n",
    "        self.model = DecisionTreeRegressor(max_depth=10,min_samples_split=5,random_state=42)\n",
    "        \n",
    "    def predict_parking_spots(self, target_lat, target_lon, time, max_distance=1.0):\n",
    "        \"\"\"\n",
    "        Predict best parking spots and their confidence scores\n",
    "        \n",
    "        Parameters:\n",
    "        - target_lat: float, destination latitude\n",
    "        - target_lon: float, destination longitude\n",
    "        - time: str, time of day (e.g., '8a')\n",
    "        - max_distance: float, maximum walking distance in km\n",
    "        \n",
    "        Returns: DataFrame with predictions and confidence scores\n",
    "        \"\"\"\n",
    "        if time not in self.time_columns:\n",
    "            raise ValueError(f\"Time must be one of {self.time_columns}\")\n",
    "        \n",
    "        time_idx = self.time_columns.index(time)\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_features = []\n",
    "        valid_meters = []\n",
    "        \n",
    "        for idx, meter in self.meter_data.iterrows():\n",
    "            # Calculate distance to destination\n",
    "            dist = self.calculate_distance(target_lat, target_lon, meter['y'], meter['x'])\n",
    "            \n",
    "            if dist <= max_distance:\n",
    "                # Find nearest traffic sensors\n",
    "                sensor_distances = []\n",
    "                traffic_values = []\n",
    "                \n",
    "                for _, sensor in self.traffic_data.iterrows():\n",
    "                    sensor_dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                                        sensor['Latitude'], sensor['Longitude'])\n",
    "                    sensor_distances.append(sensor_dist)\n",
    "                    traffic_values.append(sensor[time])\n",
    "                \n",
    "                # Get 3 nearest sensors\n",
    "                nearest_indices = np.argsort(sensor_distances)[:3]\n",
    "                \n",
    "                feature_row = [\n",
    "                    meter['x'],\n",
    "                    meter['y'],\n",
    "                    sensor_distances[nearest_indices[0]],\n",
    "                    sensor_distances[nearest_indices[1]],\n",
    "                    sensor_distances[nearest_indices[2]],\n",
    "                    traffic_values[nearest_indices[0]],\n",
    "                    traffic_values[nearest_indices[1]],\n",
    "                    traffic_values[nearest_indices[2]],\n",
    "                    time_idx\n",
    "                ]\n",
    "                \n",
    "                pred_features.append(feature_row)\n",
    "                valid_meters.append(idx)\n",
    "        \n",
    "        if not pred_features:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Scale features and make predictions\n",
    "        X_pred = self.scaler.transform(np.array(pred_features))\n",
    "        predictions = self.model.predict(X_pred)\n",
    "        \n",
    "        # Use tree depth as proxy for confidence\n",
    "        confidences = []\n",
    "        for x in X_pred:\n",
    "            # Calculate path length for this prediction\n",
    "            path_length = 0\n",
    "            node_id = 0\n",
    "            while node_id != -1:  # Until we reach a leaf\n",
    "                if self.model.tree_.feature[node_id] == -2:  # Leaf node\n",
    "                    break\n",
    "                if x[self.model.tree_.feature[node_id]] <= self.model.tree_.threshold[node_id]:\n",
    "                    node_id = self.model.tree_.children_left[node_id]\n",
    "                else:\n",
    "                    node_id = self.model.tree_.children_right[node_id]\n",
    "                path_length += 1\n",
    "            \n",
    "            # Normalize confidence based on max possible depth\n",
    "            confidence = path_length / self.model.get_depth()\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = self.meter_data.iloc[valid_meters].copy()\n",
    "        results['prediction'] = predictions\n",
    "        results['confidence'] = confidences\n",
    "        results['distance_to_dest'] = [\n",
    "            self.calculate_distance(target_lat, target_lon, row['y'], row['x'])\n",
    "            for _, row in results.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Filter by confidence threshold and sort\n",
    "        results = results[results['confidence'] >= self.confidence_threshold]\n",
    "        results = results.sort_values('prediction', ascending=False)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def create_map(self, target_lat, target_lon, recommendations):\n",
    "        \"\"\"Create a folium map with the recommended parking spots\"\"\"\n",
    "        # Create base map centered on target location\n",
    "        m = folium.Map(location=[target_lat, target_lon], zoom_start=15)\n",
    "        \n",
    "        # Add destination marker\n",
    "        folium.Marker(\n",
    "            [target_lat, target_lon],\n",
    "            popup='Destination',\n",
    "            icon=folium.Icon(color='red', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add recommended parking spots\n",
    "        for _, spot in recommendations.iterrows():\n",
    "            # Color based on confidence (green=high, orange=not high)\n",
    "            color = 'green' if spot['confidence'] > 0.7 else 'orange'\n",
    "            \n",
    "            # Create popup content\n",
    "            popup_html = f\"\"\"\n",
    "                <b>{spot['location']}</b><br>\n",
    "                Rate: {spot['rate']}<br>\n",
    "                Distance: {spot['distance_to_dest']:.2f} km<br>\n",
    "                Confidence: {spot['confidence']:.2f}<br>\n",
    "                Score: {spot['prediction']:.2f}\n",
    "            \"\"\"\n",
    "\n",
    "            folium.Marker(\n",
    "                [spot['y'], spot['x']],\n",
    "                popup=popup_html,\n",
    "                icon=folium.Icon(color=color, icon='parking', prefix='fa')\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add search box\n",
    "        plugins.Geocoder().add_to(m)\n",
    "        \n",
    "        # Add layer control\n",
    "        folium.LayerControl().add_to(m)\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def create_interactive_map(self):\n",
    "        global clicked_coords\n",
    "        clicked_coords = []\n",
    "        \n",
    "        # Create base map focused on Pittsburgh\n",
    "        m = folium.Map(\n",
    "            location=[40.4406, -79.9959],\n",
    "            zoom_start=12\n",
    "        )\n",
    "        \n",
    "        m.add_child(folium.LatLngPopup())\n",
    "        # Add clickable marker functionality\n",
    "        m.add_child(folium.ClickForLatLng())\n",
    "        \n",
    "        return m\n",
    "\n",
    "DTree = DecisionTree()\n",
    "\n",
    "# Load data\n",
    "DTree.load_data(traffic, meters)\n",
    "\n",
    "# Train model\n",
    "DTree.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I chose a KNN model because they are simple to implement from my existing structure, and update with new data well. Because traffic data is always being collected, this model can be expanded later to use that new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self):\n",
    "        self.traffic_data = None\n",
    "        self.meter_data = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.confidence_threshold = 0.6  # Minimum confidence to show recommendation\n",
    "        \n",
    "    def load_data(self, traffic_df, meters_df):\n",
    "        \"\"\"Load and prepare the traffic and parking meter datasets\"\"\"\n",
    "        self.traffic_data = traffic_df\n",
    "        self.meter_data = meters_df\n",
    "        \n",
    "        self.time_columns = ['1a','2a','3a','4a','5a','6a','7a', '8a', '9a', '10a', '11a', '12p','1p','2p','3p','4p','5p','6p','7p','8p','9p','10p','11p','12a']\n",
    "        \n",
    "        print(f\"Loaded {len(self.traffic_data)} traffic sensors and {len(self.meter_data)} parking meters\")\n",
    "        \n",
    "    def calculate_distance(self, start_latitude, start_longitude, destination_latitude, destination_longitude):\n",
    "        \"\"\"\n",
    "        Calculate distance between two points using Haversine formula\n",
    "        \n",
    "        a = sin²(φB - φA/2) + cos φA * cos φB * sin²(λB - λA/2)\n",
    "        c = 2 * atan2( √a, √(1−a) )\n",
    "        d = R ⋅ c\n",
    "        \n",
    "        \"\"\"     \n",
    "        r = 6371  # Earth's radius in kilometers\n",
    "\n",
    "        start_latitude, start_longitude, destination_latitude, destination_longitude = map(math.radians, [start_latitude, start_longitude, destination_latitude, destination_longitude])\n",
    "        delta_latitude = destination_latitude - start_latitude\n",
    "        delta_longitude = destination_longitude - start_longitude\n",
    "        \n",
    "        haversine_component = math.sin(delta_latitude/2)**2 + math.cos(start_latitude) * math.cos(destination_latitude) * math.sin(delta_longitude/2)**2\n",
    "        central_angle = 2 * math.asin(math.sqrt(haversine_component))\n",
    "        return r * central_angle\n",
    "\n",
    "    def prepare_features(self):\n",
    "        \"\"\"Prepare features for the machine learning model\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, meter in self.meter_data.iterrows():\n",
    "            # Find nearest traffic sensors\n",
    "            distances = []\n",
    "            traffic_values = []\n",
    "            \n",
    "            for _, sensor in self.traffic_data.iterrows():\n",
    "                dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                            sensor['Latitude'], sensor['Longitude'])\n",
    "                distances.append(dist)\n",
    "                \n",
    "                # Get traffic values for all time periods\n",
    "                time_values = [sensor[time_col] for time_col in self.time_columns]\n",
    "                traffic_values.append(time_values)\n",
    "            \n",
    "            # Get 3 nearest sensors\n",
    "            nearest_indices = np.argsort(distances)[:3]\n",
    "            \n",
    "            for time_idx, time_col in enumerate(self.time_columns):\n",
    "                # Features for this time period\n",
    "                feature_row = [\n",
    "                    meter['x'],  # longitude\n",
    "                    meter['y'],  # latitude\n",
    "                    distances[nearest_indices[0]],  # distance to nearest sensor\n",
    "                    distances[nearest_indices[1]],  # distance to 2nd nearest\n",
    "                    distances[nearest_indices[2]],  # distance to 3rd nearest\n",
    "                    traffic_values[nearest_indices[0]][time_idx],  # nearest sensor traffic\n",
    "                    traffic_values[nearest_indices[1]][time_idx],  # 2nd nearest traffic\n",
    "                    traffic_values[nearest_indices[2]][time_idx],  # 3rd nearest traffic\n",
    "                    time_idx  # time of day encoded as 0-5\n",
    "                ]\n",
    "                \n",
    "                features.append(feature_row)\n",
    "                \n",
    "                # Label: inverse of average traffic (higher is better for parking)\n",
    "                avg_traffic = np.mean([traffic_values[i][time_idx] for i in nearest_indices])\n",
    "                parking_score = 1.0 / (1.0 + avg_traffic/100)  # Normalize to 0-1\n",
    "                labels.append(parking_score)\n",
    "        \n",
    "        return np.array(features), np.array(labels)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the KNN model\"\"\"\n",
    "        print(\"Preparing features and training model...\")\n",
    "        \n",
    "        X, y = self.prepare_features()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "                \n",
    "        # Initialize and train KNN\n",
    "        self.model = KNeighborsRegressor(n_neighbors=5,weights='distance',metric='euclidean')\n",
    "        \n",
    "    def predict_parking_spots(self, target_lat, target_lon, time, max_distance=1.0):\n",
    "        \"\"\"\n",
    "        Predict best parking spots and their confidence scores\n",
    "        \n",
    "        Parameters:\n",
    "        - target_lat: float, destination latitude\n",
    "        - target_lon: float, destination longitude\n",
    "        - time: str, time of day (e.g., '8a')\n",
    "        - max_distance: float, maximum walking distance in km\n",
    "        \n",
    "        Returns: DataFrame with predictions and confidence scores\n",
    "        \"\"\"\n",
    "        if time not in self.time_columns:\n",
    "            raise ValueError(f\"Time must be one of {self.time_columns}\")\n",
    "        \n",
    "        time_idx = self.time_columns.index(time)\n",
    "        \n",
    "        # Prepare prediction data\n",
    "        pred_features = []\n",
    "        valid_meters = []\n",
    "        \n",
    "        for idx, meter in self.meter_data.iterrows():\n",
    "            # Calculate distance to destination\n",
    "            dist = self.calculate_distance(target_lat, target_lon, meter['y'], meter['x'])\n",
    "            \n",
    "            if dist <= max_distance:\n",
    "                # Find nearest traffic sensors\n",
    "                sensor_distances = []\n",
    "                traffic_values = []\n",
    "                \n",
    "                for _, sensor in self.traffic_data.iterrows():\n",
    "                    sensor_dist = self.calculate_distance(meter['y'], meter['x'], \n",
    "                                                        sensor['Latitude'], sensor['Longitude'])\n",
    "                    sensor_distances.append(sensor_dist)\n",
    "                    traffic_values.append(sensor[time])\n",
    "                \n",
    "                # Get 3 nearest sensors\n",
    "                nearest_indices = np.argsort(sensor_distances)[:3]\n",
    "                \n",
    "                feature_row = [\n",
    "                    meter['x'],\n",
    "                    meter['y'],\n",
    "                    sensor_distances[nearest_indices[0]],\n",
    "                    sensor_distances[nearest_indices[1]],\n",
    "                    sensor_distances[nearest_indices[2]],\n",
    "                    traffic_values[nearest_indices[0]],\n",
    "                    traffic_values[nearest_indices[1]],\n",
    "                    traffic_values[nearest_indices[2]],\n",
    "                    time_idx\n",
    "                ]\n",
    "                \n",
    "                pred_features.append(feature_row)\n",
    "                valid_meters.append(idx)\n",
    "        \n",
    "        if not pred_features:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Scale features and make predictions\n",
    "        X_pred = self.scaler.transform(np.array(pred_features))\n",
    "\n",
    "        # Get predictions and distances to nearest neighbors\n",
    "        predictions = self.model.predict(X_pred)\n",
    "        distances, _ = self.model.kneighbors(X_pred)\n",
    "\n",
    "        # Convert distances to confidence scores (closer = more confident)\n",
    "        max_dist = np.max(distances)\n",
    "        confidences = 1 - np.mean(distances / max_dist, axis=1)\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results = self.meter_data.iloc[valid_meters].copy()\n",
    "        results['prediction'] = predictions\n",
    "        results['confidence'] = confidences\n",
    "        results['distance_to_dest'] = [\n",
    "            self.calculate_distance(target_lat, target_lon, row['y'], row['x'])\n",
    "            for _, row in results.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Filter by confidence threshold and sort\n",
    "        results = results[results['confidence'] >= self.confidence_threshold]\n",
    "        results = results.sort_values('prediction', ascending=False)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def create_map(self, target_lat, target_lon, recommendations):\n",
    "        \"\"\"Create a folium map with the recommended parking spots\"\"\"\n",
    "        # Create base map centered on target location\n",
    "        m = folium.Map(location=[target_lat, target_lon], zoom_start=15)\n",
    "        \n",
    "        # Add destination marker\n",
    "        folium.Marker(\n",
    "            [target_lat, target_lon],\n",
    "            popup='Destination',\n",
    "            icon=folium.Icon(color='red', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add recommended parking spots\n",
    "        for _, spot in recommendations.iterrows():\n",
    "            # Color based on confidence (green=high, orange=not high)\n",
    "            color = 'green' if spot['confidence'] > 0.7 else 'orange'\n",
    "            \n",
    "            # Create popup content\n",
    "            popup_html = f\"\"\"\n",
    "                <b>{spot['location']}</b><br>\n",
    "                Rate: {spot['rate']}<br>\n",
    "                Distance: {spot['distance_to_dest']:.2f} km<br>\n",
    "                Confidence: {spot['confidence']:.2f}<br>\n",
    "                Score: {spot['prediction']:.2f}\n",
    "            \"\"\"\n",
    "\n",
    "            folium.Marker(\n",
    "                [spot['y'], spot['x']],\n",
    "                popup=popup_html,\n",
    "                icon=folium.Icon(color=color, icon='parking', prefix='fa')\n",
    "            ).add_to(m)\n",
    "        \n",
    "        # Add search box\n",
    "        plugins.Geocoder().add_to(m)\n",
    "        \n",
    "        # Add layer control\n",
    "        folium.LayerControl().add_to(m)\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def create_interactive_map(self):\n",
    "        global clicked_coords\n",
    "        clicked_coords = []\n",
    "        \n",
    "        # Create base map focused on Pittsburgh\n",
    "        m = folium.Map(\n",
    "            location=[40.4406, -79.9959],\n",
    "            zoom_start=12\n",
    "        )\n",
    "        \n",
    "        m.add_child(folium.LatLngPopup())\n",
    "        # Add clickable marker functionality\n",
    "        m.add_child(folium.ClickForLatLng())\n",
    "        \n",
    "        return m\n",
    "\n",
    "knnModel = KNN()\n",
    "\n",
    "# Load data\n",
    "knnModel.load_data(traffic, meters)\n",
    "\n",
    "# Train model\n",
    "knnModel.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Interactable Model - Random Forest Regression Model Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global clicked_coords\n",
    "clicked_coords = []\n",
    "\n",
    "while(True):\n",
    "    initial_map = RandForest.create_interactive_map()\n",
    "    display(initial_map)\n",
    "    \n",
    "    user_input = input(\"After placing marker, copy the coordinates as input to the model. Press Enter to continue...\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    clicked_coords = user_input.split(sep=' ') # Default to Pittsburgh\n",
    "    \n",
    "    if clicked_coords:\n",
    "        lat = float(clicked_coords[1])\n",
    "        lon = float(clicked_coords[3])\n",
    "        user_time = input(\"Enter a time in the formate <hour><a/p> (e.g., 8a): \")\n",
    "        max_distance = float(input(\"Enter maximum walking distance (km): \"))\n",
    "        if not user_time or not max_distance:\n",
    "            print(\"Invalid input. Please try again.\")\n",
    "            continue\n",
    "        \n",
    "        # Make prediction\n",
    "        recommendations = RandForest.predict_parking_spots(lat, lon, user_time, max_distance)\n",
    "        break\n",
    "    else:\n",
    "        print(\"No location selected. Please try again.\")\n",
    "        \n",
    "\n",
    "print(\"\\nTop recommendations:\")\n",
    "print(recommendations[['location', 'rate', 'distance_to_dest', 'prediction', 'confidence']].head())\n",
    "\n",
    "# Create and save map\n",
    "m = RandForest.create_map(lat, lon, recommendations)\n",
    "m.save('parking_recommendations.html')\n",
    "print(\"\\nMap saved as 'parking_recommendations.html'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model from my testing is the random forest regression model. Its ability to handle skewed and non-linear data well made it the best-performing model overall. \n",
    "\n",
    "\n",
    "This model outputs it's top 5 predicted best parking spots based off predicted occupancy (on a scale of 0-1) and its confidence in its prediction. Using the 'recommendation' data from the model, an interactive map is created that shows all possible meter locations within the user-defined distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage: \n",
    "* Click \"Run All\"\n",
    "* Wait for all of the models to train and load (takes a while, but I figured they should all at least run)\n",
    "* Once the interactive map pops up, find a location within the city to park and click on it. (Ex. The Pete)\n",
    "* Copy all of the text shown on the map pin and paste it into terminal asking for user input.\n",
    "* Enter a time into the user-input terminal\n",
    "* Enter a max walking distance in kilometers (float)\n",
    "* Look at the recommendations or run the html file generated to get a map of parking meter suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources/Help During Project\n",
    "\n",
    "Anthropic Claude for Helping with Some of the Boilerplate Code\n",
    "* Code Comments Between Each Model\n",
    "* Calculating Distance in Circle\n",
    "* Specific Styling Choices for the Folium Map\n",
    "\n",
    "Folium Guide For How to Use the Interactive Map\n",
    "* https://python-visualization.github.io/folium/latest/user_guide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
